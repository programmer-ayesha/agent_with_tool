{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfH69r5ast6gefEroVE2Tn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/programmer-ayesha/agent_with_tool/blob/main/tools_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fXD1itlYEtks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5960811d-462e-413d-e641-dbbb92f16954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.5/168.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip  install -Uq  openai-agents 'openai-agents[litellm]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "AKdzeN6QksdS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_on_first_tool\n",
        "from agents import Agent, Runner, function_tool,StopAtTools,ModelSettings\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = \"gemini/gemini-2.0-flash\"\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "@function_tool\n",
        "def news(topic:str)->str:\n",
        "  return \"The latest news on {} is ....\".format(topic)\n",
        "\n",
        "@function_tool\n",
        "def main(city:str)->str:\n",
        "  \"\"\"\n",
        "  this tool returns the weather in a city\n",
        "  \"\"\"\n",
        "  return \"THE WEATHER IN {} IS RAINING\".format(city)\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You are a help full assistant that answers questions. you have a tool for weather and news query you will Always use tool\",\n",
        "      model = LitellmModel(model=MODEL , api_key=GOOGLE_API_KEY),\n",
        "      tools=[main,news],\n",
        "      # tool_use_behavior=StopAtTools(stop_at_tool_names=[\"main\"])\n",
        "      model_settings=ModelSettings(tool_choice=\"required\")\n",
        ")\n",
        "result =Runner.run_sync(agent, \"what is the news topic agentic ai what is the weather in paris?\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0e7KexWkWiY",
        "outputId": "07bfbbc1-065b-4ac4-856b-cd201dad5e58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. The weather in Paris is raining. The latest news on agentic ai is ....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tool Choice\n",
        "\n",
        "      model_settings=ModelSettings(tool_choice=\"required\"),\n",
        "\n",
        "with different user query"
      ],
      "metadata": {
        "id": "sWEkscwtlJwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, function_tool,StopAtTools,ModelSettings,set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "\n",
        "MODEL = \"gemini/gemini-2.0-flash\"\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "@function_tool\n",
        "def news(topic:str)->str:\n",
        "  print(\"tool news is calling\")\n",
        "  return \"The latest news on {} is ....\".format(topic)\n",
        "\n",
        "@function_tool\n",
        "def main(city:str)->str:\n",
        "  print(\"tool weather is calling\")\n",
        "  \"\"\"\n",
        "  this tool returns the weather in a city\n",
        "  \"\"\"\n",
        "  return \"THE WEATHER IN {} IS RAINING\".format(city)\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You are a help full assistant that answers questions. you have a tool for weather and news query you will Always use tool\",\n",
        "      model = LitellmModel(model=MODEL , api_key=GOOGLE_API_KEY),\n",
        "      tools=[main,news],\n",
        "      model_settings=ModelSettings(tool_choice=\"required\"),\n",
        ")\n",
        "result =Runner.run_sync(agent, \"hi?\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zePbycZ6kvhQ",
        "outputId": "a3c2952b-3fa5-476a-88bf-e2647ab4b64b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool weather is calling\n",
            "Hi! It's raining in London. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reset Tool Choice\n",
        "\n",
        "      reset_tool_choice=False\n"
      ],
      "metadata": {
        "id": "XTvaWqobnmsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, function_tool,StopAtTools,ModelSettings\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "\n",
        "MODEL = \"gemini/gemini-2.0-flash\"\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "@function_tool\n",
        "def news(topic:str)->str:\n",
        "  return \"The latest news on {} is ....\".format(topic)\n",
        "\n",
        "@function_tool\n",
        "def main(city:str)->str:\n",
        "  \"\"\"\n",
        "  this tool returns the weather in a city\n",
        "  \"\"\"\n",
        "  return \"THE WEATHER IN {} IS RAINING\".format(city)\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You are a help full assistant that answers questions. you have a tool for weather and news query you will Always use tool\",\n",
        "      model = LitellmModel(model=MODEL , api_key=GOOGLE_API_KEY),\n",
        "      tools=[main,news],\n",
        "      model_settings=ModelSettings(tool_choice=\"required\"),\n",
        "      reset_tool_choice=False\n",
        ")\n",
        "result =Runner.run_sync(agent, \"hi?\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Zm9dGQW3mLhy",
        "outputId": "065666f5-19d2-4ce1-ea00-684f79a02b86"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MaxTurnsExceeded",
          "evalue": "Max turns (10) exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3463731984.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mreset_tool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hi?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun_sync\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"\n\u001b[1;32m    290\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_AGENT_RUNNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         return runner.run_sync(\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun_sync\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"session\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         return asyncio.get_event_loop().run_until_complete(\n\u001b[0m\u001b[1;32m    545\u001b[0m             self.run(\n\u001b[1;32m    546\u001b[0m                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                             ),\n\u001b[1;32m    435\u001b[0m                         )\n\u001b[0;32m--> 436\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mMaxTurnsExceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max turns ({max_turns}) exceeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     logger.debug(\n",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m: Max turns (10) exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tool_choice=\"none\""
      ],
      "metadata": {
        "id": "_Ijm-HyRqeZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, function_tool,StopAtTools,ModelSettings\n",
        "\n",
        "MODEL = \"gemini/gemini-2.0-flash\"\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "@function_tool\n",
        "def news(topic:str)->str:\n",
        "  return \"The latest news on {} is ....\".format(topic)\n",
        "\n",
        "@function_tool\n",
        "def main(city:str)->str:\n",
        "  \"\"\"\n",
        "  this tool returns the weather in a city\n",
        "  \"\"\"\n",
        "  return \"THE WEATHER IN {} IS RAINING\".format(city)\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You Always use tool for news\",\n",
        "      model = LitellmModel(model=MODEL , api_key=GOOGLE_API_KEY),\n",
        "      tools=[main,news],\n",
        "      tool_use_behavior='stop_on_first_tool',\n",
        "      model_settings=ModelSettings(tool_choice=\"none\"),\n",
        ")\n",
        "result =Runner.run_sync(agent, \"what is the new topic is Agentic AI\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivw6eCX8p-dJ",
        "outputId": "4f56ac3f-c822-4ffd-9f0e-fda26b6944d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, to get the latest news and information on Agentic AI, I'll use my tools to search for recent articles and discussions. After that, I will summarise the key aspects of this emerging field.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tool_choice=none"
      ],
      "metadata": {
        "id": "ql5T4hwyqxbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, function_tool,StopAtTools,ModelSettings\n",
        "\n",
        "MODEL = \"gemini/gemini-2.0-flash\"\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "@function_tool\n",
        "def news(topic:str)->str:\n",
        "  return \"The latest news on {} is ....\".format(topic)\n",
        "\n",
        "@function_tool\n",
        "def main(city:str)->str:\n",
        "  \"\"\"\n",
        "  this tool returns the weather in a city\n",
        "  \"\"\"\n",
        "  return \"THE WEATHER IN {} IS RAINING\".format(city)\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You Always use tool for news\",\n",
        "      model = LitellmModel(model=MODEL , api_key=GOOGLE_API_KEY),\n",
        "      tools=[main,news],\n",
        "      tool_use_behavior='stop_on_first_tool',\n",
        "      model_settings=ModelSettings(tool_choice=\"auto\"),\n",
        ")\n",
        "result =Runner.run_sync(agent, \"what is the new topic is Agentic AI\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvkRgD6Jqoee",
        "outputId": "cf75972c-f490-40fe-f0c9-bc30fa1bada7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The latest news on Agentic AI is ....\n"
          ]
        }
      ]
    }
  ]
}